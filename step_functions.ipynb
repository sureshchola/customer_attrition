
   "source": [
    "!pip install stepfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c85c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661e6c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process features step\n",
      "training model step\n",
      "creating model step\n",
      "endpoint config step\n",
      "endpoint step\n",
      "success step\n",
      "chain steps\n",
      "create and execute workflow\n",
      "updating workflow\n",
      "executing workflow\n",
      "waiting for output\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "from stepfunctions import steps,inputs, workflow\n",
    "from sagemaker.processing import FrameworkProcessor, ProcessingOutput, ProcessingInput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "import os\n",
    "from datetime import datetime\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "sm_client = boto3.client('sagemaker', region_name='us-east-1')\n",
    "ssm_client = boto3.client('ssm', region_name='us-east-1')\n",
    "\n",
    "def training_pipeline(model,env,processing_script,processing_tar,training_script,training_tar):\n",
    "    \n",
    "    \"\"\"\n",
    "    create or update a step functions training pipeline for specific model and environment\n",
    "    \n",
    "    Args:\n",
    "    model - model name\n",
    "    env - environment\n",
    "    processing_script - script to execute processing\n",
    "    processing_tar - tar file to unzip that contains processing code\n",
    "    training_script - script to execute training\n",
    "    training_tar - tar file to unzip that contains training code\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    code_s3 = ssm_client.get_parameter(Name=f'/{model}/{env}/code_s3')['Parameter']['Value'] #\n",
    "    secret_name = ssm_client.get_parameter(Name=f'/{env}/secret_name')['Parameter']['Value'] #\n",
    "    region = ssm_client.get_parameter(Name='region')['Parameter']['Value'] #\n",
    "    account = ssm_client.get_parameter(Name='account')['Parameter']['Value'] #\n",
    "    subnets = ssm_client.get_parameter(Name='subnets')['Parameter']['Value'] #\n",
    "    sgid = ssm_client.get_parameter(Name='sgid')['Parameter']['Value'] #\n",
    "    sm_output_s3 = ssm_client.get_parameter(Name=f'/{model}/{env}/sm_output_s3')['Parameter']['Value'] #\n",
    "    endpoint_name = ssm_client.get_parameter(Name=f'/{model}/{env}/endpoint_name')['Parameter']['Value'] #\n",
    "\n",
    "    # create a string suffix for utc datetime i.g. 2021-01-01-10-31-255\n",
    "    utc_dt = str(datetime.now())[:23]\n",
    "    chars = ':. '\n",
    "    for char in chars:\n",
    "        utc_dt = utc_dt.replace(char, '-')\n",
    "\n",
    "    # placeholders for dynamic input for workflow inputs\n",
    "    execution_input = inputs.ExecutionInput(schema={\"ProcessingJobName\": str,\n",
    "                                             \"TrainingJobName\": str, \n",
    "                                             \"ModelName\":str,\n",
    "                                             \"EndpointConfigName\":str,\n",
    "                                             \"EndpointName\":str})\n",
    "    \n",
    "    # create payload for lambda\n",
    "    def create_payload(pipeline,state='failed',body='Please check logs for details'):\n",
    "        return {\"region\": region,\n",
    "                \"pipeline\": f\"{model}:pipeline\",\n",
    "                \"account\": account,\n",
    "                \"env\": env,\n",
    "                \"state\": state,\n",
    "                \"body\": body}\n",
    "    \n",
    "    # process features step\n",
    "    print(\"process features step\")\n",
    "    nc = NetworkConfig(subnets=subnets.split(','),\n",
    "                      security_group_ids=[sgid])\n",
    "\n",
    "    fw_processor = FrameworkProcessor(\n",
    "        estimator_cls = SKLearn,\n",
    "        framework_version=\"0.20.0\",\n",
    "        role=get_execution_role(),\n",
    "        instance_type=\"ml.m4.2xlarge\",\n",
    "        instance_count=1,\n",
    "        network_config=nc)\n",
    "\n",
    "    processing_step =steps.ProcessingStep(\"process features\",\n",
    "                                    fw_processor,\n",
    "                                    job_name=execution_input['ProcessingJobName'],\n",
    "                                    container_entrypoint=[\"/bin/bash\", f\"/opt/ml/processing/input/entrypoint/{processing_script}\"],\n",
    "                                    inputs = [ProcessingInput(source=\"/\".join([code_s3,processing_tar]),\n",
    "                                                              input_name='code',\n",
    "                                                              destination=\"/opt/ml/processing/input/code\"),\n",
    "                                             ProcessingInput(source=\"/\".join([code_s3,processing_script]),\n",
    "                                                              input_name='entrypoint',\n",
    "                                                              destination=\"/opt/ml/processing/input/entrypoint\")],\n",
    "                                    outputs=[ProcessingOutput(output_name='output-1',\n",
    "                                                              source='/opt/ml/processing/output/entrypoint',\n",
    "                                                             destination=sm_output_s3)],\n",
    "                                    wait_for_completion=True)\n",
    "\n",
    "    processing_step.add_catch(steps.states.Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                    next_step=steps.LambdaStep(\"process features failed\",\n",
    "                                                        parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                    \"Payload\": create_payload(\"process features\")})))\n",
    "\n",
    "    # training model step\n",
    "    print(\"training model step\")\n",
    "    estimator = XGBoost(framework_version='1.0-1',\n",
    "                        source_dir =  os.path.join(code_s3,training_tar),\n",
    "                        entry_point = training_script,\n",
    "                        hyperparameters = {'secret_name': secret_name,\n",
    "                                          'region': region,\n",
    "                                          'account': account,\n",
    "                                          'env': env},\n",
    "                        role = get_execution_role(),\n",
    "                        instance_count=1,\n",
    "                        instance_type='ml.m4.2xlarge',\n",
    "                        subnets=subnets.split(','),\n",
    "                        security_group_ids=[sgid],\n",
    "                        output_path=sm_output_s3)\n",
    "\n",
    "    training_step = steps.TrainingStep(\"training model\",\n",
    "                                estimator=estimator,\n",
    "                                wait_for_completion=True,\n",
    "                                job_name=execution_input['TrainingJobName'])\n",
    "\n",
    "    training_step.add_catch(steps.states.Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                    next_step=steps.LambdaStep(\"training model failed\",\n",
    "                                                        parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                    \"Payload\": create_payload(\"training model\")})))\n",
    "    # creating model step\n",
    "    print(\"creating model step\")\n",
    "    model_step = steps.ModelStep(\"creating model\", \n",
    "                            model=training_step.get_expected_model(), \n",
    "                            model_name=execution_input[\"ModelName\"])\n",
    "\n",
    "    model_step.add_catch(steps.states.Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                    next_step=steps.LambdaStep(\"creating model failed\",\n",
    "                                                        parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                    \"Payload\": create_payload(\"creating model\")})))\n",
    "\n",
    "    # endpoint config step\n",
    "    print(\"endpoint config step\")\n",
    "    endpoint_config_step = steps.EndpointConfigStep(\"endpoint config step\",\n",
    "                                            endpoint_config_name=execution_input[\"EndpointConfigName\"],\n",
    "                                            model_name=execution_input[\"ModelName\"],\n",
    "                                            initial_instance_count=1,\n",
    "                                            instance_type=\"ml.m5.large\")\n",
    "\n",
    "    endpoint_config_step.add_catch(steps.states.Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                    next_step=steps.LambdaStep(\"endpoint config failed\",\n",
    "                                                        parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                    \"Payload\": create_payload(\"create endpoint config\")})))\n",
    "\n",
    "    # endpoint step\n",
    "    print(\"endpoint step\")\n",
    "    endpoint_list = sm_client.list_endpoints()\n",
    "    if any(endpoint['EndpointName']==f'{model}-{env}' for endpoint in endpoint_list['Endpoints']):\n",
    "        update=True\n",
    "    else:\n",
    "        update=False\n",
    "    \n",
    "    endpoint_step = steps.EndpointStep(\"endpoint step\",\n",
    "                                endpoint_name=execution_input[\"EndpointName\"],\n",
    "                                endpoint_config_name=execution_input[\"EndpointConfigName\"],\n",
    "                                update=update)\n",
    "\n",
    "    endpoint_step.add_catch(steps.states.Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                    next_step=steps.LambdaStep(\"endpoint failed\",\n",
    "                                                        parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                    \"Payload\": create_payload(\"create endpoint\")})))\n",
    "    \n",
    "    # success step\n",
    "    print(\"success step\")\n",
    "    success_step = steps.LambdaStep(\"training pipeline succeeded\",\n",
    "                                parameters={\"FunctionName\": \"notifications\",\n",
    "                                            \"Payload\": create_payload(\"training pipeline\",\"succeeded\",'')})\n",
    "    \n",
    "    # chain steps together\n",
    "    print(\"chain steps\")\n",
    "    workflow_definition = steps.Chain(\n",
    "        [processing_step,training_step,model_step,endpoint_config_step,endpoint_step,success_step]\n",
    "    )\n",
    "\n",
    "    # create and execute workflow\n",
    "    print(\"create and execute workflow\")\n",
    "    workflow_list = workflow.Workflow.list_workflows()\n",
    "    found=False\n",
    "    for wflow in workflow_list:\n",
    "        if wflow['name'] == f'{model}-{env}-training-pipeline':\n",
    "            print(\"updating workflow\")\n",
    "            wf = workflow.Workflow(\n",
    "                name=f'{model}-{env}-training-pipeline',\n",
    "                definition=workflow_definition,\n",
    "                role=get_execution_role(),\n",
    "                execution_input=execution_input,\n",
    "                state_machine_arn=wflow['stateMachineArn']\n",
    "            )\n",
    "            wf.update(definition=workflow_definition)\n",
    "            found=True\n",
    "            break\n",
    "    if not found:\n",
    "        print(\"creating workflow\")\n",
    "        wf = workflow.Workflow(\n",
    "            name=f'{model}-{env}-training-pipeline',\n",
    "            definition=workflow_definition,\n",
    "            role=get_execution_role(),\n",
    "            execution_input=execution_input\n",
    "        )\n",
    "        wf.create()\n",
    "\n",
    "    # sleep to make sure workflow has been updated\n",
    "    time.sleep(30)\n",
    "    print(\"executing workflow\")\n",
    "    execution = wf.execute(inputs={\n",
    "            \"ProcessingJobName\": f'{model}-{env}-processing-business-{utc_dt}',\n",
    "            \"TrainingJobName\": f'{model}-{env}-training-{utc_dt}',\n",
    "            \"ModelName\": f'{model}-{env}-model-{utc_dt}',\n",
    "            \"EndpointConfigName\": f'{model}-{env}-config-{utc_dt}',\n",
    "            \"EndpointName\": f'{model}-{env}'})\n",
    "\n",
    "    print(\"waiting for output\")\n",
    "    execution.get_output(wait=True)\n",
    "    \n",
    "if __name__=='__main__':\n",
    "    training_pipeline('attrition','dev','features_runproc.sh','core.tar.gz','train_business_model.py','src.tar.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1180da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing step 1\n",
      "processing step 2\n",
      "success step\n",
      "created workflow\n",
      "executing workflow\n",
      "waiting for output\n"
     ]
    }
   ],
   "source": [
    "# inference\n",
    "\n",
    "from stepfunctions.steps import TrainingStep, ModelStep, ProcessingStep, EndpointStep, EndpointConfigStep, Chain\n",
    "from stepfunctions.steps import LambdaStep\n",
    "from stepfunctions.steps.states import Catch,Pass\n",
    "from stepfunctions.inputs import StepInput\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "from sagemaker.processing import FrameworkProcessor, ProcessingOutput, ProcessingInput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "utc_dt = str(datetime.now())[:23]\n",
    "chars = ':. '\n",
    "for char in chars:\n",
    "    utc_dt = utc_dt.replace(char, '-')\n",
    "\n",
    "execution_input = ExecutionInput(schema={\"ProcessingJobName1\": str,\n",
    "                                         \"ProcessingJobName2\": str})\n",
    "\n",
    "print(\"processing step 1\")\n",
    "nc = NetworkConfig(subnets=os.environ['subnets'].split(','),\n",
    "                  security_group_ids=[os.environ['sgid']])\n",
    "\n",
    "\n",
    "fw_processor = FrameworkProcessor(\n",
    "    estimator_cls = SKLearn,\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=get_execution_role(),\n",
    "    instance_type=\"ml.m4.2xlarge\",\n",
    "    instance_count=1,\n",
    "    network_config=nc)\n",
    "\n",
    "processing_step1 =ProcessingStep(\"Processing Step 1\",\n",
    "                                fw_processor,\n",
    "                                job_name=execution_input['ProcessingJobName1'],\n",
    "                                container_entrypoint=[\"/bin/bash\", \"/opt/ml/processing/input/entrypoint/features_runproc.sh\"],\n",
    "                                inputs = [ProcessingInput(source=\"/\".join([os.environ['code_s3'],\"core.tar.gz\"]),\n",
    "                                                          input_name='code',\n",
    "                                                          destination=\"/opt/ml/processing/input/code\"),\n",
    "                                         ProcessingInput(source=\"/\".join([os.environ['code_s3'],\"features_runproc.sh\"]),\n",
    "                                                          input_name='entrypoint',\n",
    "                                                          destination=\"/opt/ml/processing/input/entrypoint\")],\n",
    "                                outputs=[ProcessingOutput(output_name='output-1',\n",
    "                                                          source='/opt/ml/processing/output/entrypoint',\n",
    "                                                         destination=os.environ['sm_output_s3'])],\n",
    "                                wait_for_completion=True)\n",
    "\n",
    "processing_step1.add_catch(Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                next_step=LambdaStep(\"processing_job1_failed\",\n",
    "                                                    parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                \"Payload\": {\"region\": \"us-east-1\",\n",
    "                                                                            \"pipeline\": \"inferencing features\",\n",
    "                                                                            \"account\": 740984199597,\n",
    "                                                                            \"env\": \"dev\",\n",
    "                                                                            \"state\": \"failed\",\n",
    "                                                                            \"body\": \"Please check logs for details\"}})))\n",
    "\n",
    "print(\"processing step 2\")\n",
    "processing_step2 =ProcessingStep(\"Processing Step 2\",\n",
    "                                fw_processor,\n",
    "                                job_name=execution_input['ProcessingJobName2'],\n",
    "                                container_entrypoint=[\"/bin/bash\", \"/opt/ml/processing/input/entrypoint/predictions_runproc.sh\"],\n",
    "                                inputs = [ProcessingInput(source=\"/\".join([os.environ['code_s3'],\"inferencing.tar.gz\"]),\n",
    "                                                          input_name='code',\n",
    "                                                          destination=\"/opt/ml/processing/input/code\"),\n",
    "                                         ProcessingInput(source=\"/\".join([os.environ['code_s3'],\"predictions_runproc.sh\"]),\n",
    "                                                          input_name='entrypoint',\n",
    "                                                          destination=\"/opt/ml/processing/input/entrypoint\")],\n",
    "                                outputs=[ProcessingOutput(output_name='output-1',\n",
    "                                                          source='/opt/ml/processing/output/entrypoint',\n",
    "                                                         destination=os.environ['sm_output_s3'])],\n",
    "                                wait_for_completion=True)\n",
    "\n",
    "processing_step2.add_catch(Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                next_step=LambdaStep(\"processing_job2_failed\",\n",
    "                                                    parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                \"Payload\": {\"region\": \"us-east-1\",\n",
    "                                                                            \"pipeline\": \"inferencing predictions\",\n",
    "                                                                            \"account\": 740984199597,\n",
    "                                                                            \"env\": \"dev\",\n",
    "                                                                            \"state\": \"failed\",\n",
    "                                                                            \"body\": \"Please check logs for details\"}})))\n",
    "\n",
    "print(\"success step\")\n",
    "success_step = LambdaStep(\"inferencing pipeline succeeded\",\n",
    "                            parameters={\"FunctionName\": \"notifications\",\n",
    "                                        \"Payload\": {\"region\": \"us-east-1\",\n",
    "                                                    \"pipeline\": \"inferencing\",\n",
    "                                                    \"account\": 740984199597,\n",
    "                                                    \"env\": \"dev\",\n",
    "                                                    \"state\": \"succeeded\",\n",
    "                                                    \"body\": \"\"}})\n",
    "\n",
    "workflow_definition = Chain(\n",
    "    [processing_step1,processing_step2,success_step]\n",
    ")\n",
    "\n",
    "workflow = Workflow(\n",
    "    name=\"inferencing2\",\n",
    "    definition=workflow_definition,\n",
    "    role=get_execution_role(),\n",
    "    execution_input=execution_input\n",
    ")\n",
    "\n",
    "# print(5)\n",
    "# workflow.update(definition=workflow_definition)\n",
    "\n",
    "print(\"created workflow\")\n",
    "workflow.create()\n",
    "\n",
    "print(\"executing workflow\")\n",
    "execution = workflow.execute(inputs={\n",
    "        \"ProcessingJobName1\": f'attrition-processing-inferencing-{utc_dt}',\n",
    "        \"ProcessingJobName2\": f'attrition-inferencing-predictions-{utc_dt}'})\n",
    "\n",
    "print(\"waiting for output\")\n",
    "execution.get_output(wait=True)\n",
    "\n",
    "# print(7)\n",
    "# print(execution.render_progress())\n",
    "\n",
    "# print(8)\n",
    "# print(execution.list_events())\n",
    "\n",
    "# print(9)\n",
    "# print(workflow.list_executions())\n",
    "\n",
    "# print(10)\n",
    "# print(Workflow.list_workflows())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "91689439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "endpoint config step\n",
      "endpoint step\n",
      "success step\n",
      "created workflow\n",
      "executing workflow\n",
      "waiting for output\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ExecutedVersion': '$LATEST',\n",
       " 'Payload': {'statusCode': 200,\n",
       "  'body': '{\"region\": \"us-east-1\", \"pipeline\": \"replace model\", \"account\": 740984199597, \"env\": \"dev\", \"state\": \"succeeded\", \"body\": \"\"}'},\n",
       " 'SdkHttpMetadata': {'AllHttpHeaders': {'X-Amz-Executed-Version': ['$LATEST'],\n",
       "   'x-amzn-Remapped-Content-Length': ['0'],\n",
       "   'Connection': ['keep-alive'],\n",
       "   'x-amzn-RequestId': ['f5ab18ce-0136-4cd9-8cb3-1a584f8876a7'],\n",
       "   'Content-Length': ['178'],\n",
       "   'Date': ['Tue, 02 Nov 2021 04:29:53 GMT'],\n",
       "   'X-Amzn-Trace-Id': ['root=1-6180bebf-446addf93dcf5d6e15771336;sampled=0'],\n",
       "   'Content-Type': ['application/json']},\n",
       "  'HttpHeaders': {'Connection': 'keep-alive',\n",
       "   'Content-Length': '178',\n",
       "   'Content-Type': 'application/json',\n",
       "   'Date': 'Tue, 02 Nov 2021 04:29:53 GMT',\n",
       "   'X-Amz-Executed-Version': '$LATEST',\n",
       "   'x-amzn-Remapped-Content-Length': '0',\n",
       "   'x-amzn-RequestId': 'f5ab18ce-0136-4cd9-8cb3-1a584f8876a7',\n",
       "   'X-Amzn-Trace-Id': 'root=1-6180bebf-446addf93dcf5d6e15771336;sampled=0'},\n",
       "  'HttpStatusCode': 200},\n",
       " 'SdkResponseMetadata': {'RequestId': 'f5ab18ce-0136-4cd9-8cb3-1a584f8876a7'},\n",
       " 'StatusCode': 200}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace model\n",
    "from stepfunctions.steps import TrainingStep, ModelStep, ProcessingStep, EndpointStep, EndpointConfigStep, Chain\n",
    "from stepfunctions.steps import LambdaStep\n",
    "from stepfunctions.steps.states import Catch,Pass\n",
    "from stepfunctions.inputs import StepInput\n",
    "from stepfunctions.inputs import ExecutionInput\n",
    "from stepfunctions.workflow import Workflow\n",
    "\n",
    "from sagemaker.processing import FrameworkProcessor, ProcessingOutput, ProcessingInput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.network import NetworkConfig\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "utc_dt = str(datetime.now())[:23]\n",
    "chars = ':. '\n",
    "for char in chars:\n",
    "    utc_dt = utc_dt.replace(char, '-')\n",
    "\n",
    "execution_input = ExecutionInput(schema={\"ModelName\":str,\n",
    "                                         \"EndpointConfigName\":str,\n",
    "                                         \"EndpointName\":str})\n",
    "\n",
    "print(\"endpoint config step\")\n",
    "endpoint_config_step = EndpointConfigStep(\n",
    "    \"Create Endpoint Config\",\n",
    "    endpoint_config_name=execution_input[\"EndpointConfigName\"],\n",
    "    model_name=execution_input[\"ModelName\"],\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\")\n",
    "\n",
    "endpoint_config_step.add_catch(Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                next_step=LambdaStep(\"endpoint_config_failed\",\n",
    "                                                    parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                \"Payload\": {\"region\": \"us-east-1\",\n",
    "                                                                            \"pipeline\": \"endpoint config\",\n",
    "                                                                            \"account\": 740984199597,\n",
    "                                                                            \"env\": \"dev\",\n",
    "                                                                            \"state\": \"failed\",\n",
    "                                                                            \"body\": \"Please check logs for details\"}})))\n",
    "\n",
    "print(\"endpoint step\")\n",
    "endpoint_step = EndpointStep(\n",
    "    \"Create Endpoint\",\n",
    "    endpoint_name=execution_input[\"EndpointName\"],\n",
    "    endpoint_config_name=execution_input[\"EndpointConfigName\"],\n",
    "    update=True\n",
    ")\n",
    "\n",
    "endpoint_step.add_catch(Catch(error_equals=[\"States.TaskFailed\"],\n",
    "                                next_step=LambdaStep(\"endpoint_failed\",\n",
    "                                                    parameters={\"FunctionName\": \"notifications\",\n",
    "                                                                \"Payload\": {\"region\": \"us-east-1\",\n",
    "                                                                            \"pipeline\": \"endpoint\",\n",
    "                                                                            \"account\": 740984199597,\n",
    "                                                                            \"env\": \"dev\",\n",
    "                                                                            \"state\": \"failed\",\n",
    "                                                                            \"body\": \"Please check logs for details\"}})))\n",
    "\n",
    "print(\"success step\")\n",
    "success_step = LambdaStep(\"replace model succeeded\",\n",
    "                            parameters={\"FunctionName\": \"notifications\",\n",
    "                                        \"Payload\": {\"region\": \"us-east-1\",\n",
    "                                                    \"pipeline\": \"replace model\",\n",
    "                                                    \"account\": 740984199597,\n",
    "                                                    \"env\": \"dev\",\n",
    "                                                    \"state\": \"succeeded\",\n",
    "                                                    \"body\": \"\"}})\n",
    "\n",
    "workflow_definition = Chain(\n",
    "    [endpoint_config_step,endpoint_step,success_step]\n",
    ")\n",
    "\n",
    "workflow = Workflow(\n",
    "    name=\"replace_model\",\n",
    "    definition=workflow_definition,\n",
    "    role=get_execution_role(),\n",
    "    execution_input=execution_input\n",
    ")\n",
    "\n",
    "# print(5)\n",
    "# workflow.update(definition=workflow_definition)\n",
    "\n",
    "print(\"created workflow\")\n",
    "workflow.create()\n",
    "\n",
    "print(\"executing workflow\")\n",
    "execution = workflow.execute(inputs={\n",
    "        \"ModelName\": 'sagemaker-xgboost-2021-09-08-16-39-26-995',\n",
    "        \"EndpointConfigName\": f'attrition-config-{utc_dt}',\n",
    "        \"EndpointName\": \"attrition-model\"})\n",
    "\n",
    "print(\"waiting for output\")\n",
    "execution.get_output(wait=True)\n",
    "\n",
    "#         \"ProcessingJobName\": f'attrition-processing-training-{utc_dt}',\n",
    "# print(7)\n",
    "# print(execution.render_progress())\n",
    "\n",
    "# print(8)\n",
    "# print(execution.list_events())\n",
    "\n",
    "# print(9)\n",
    "# print(workflow.list_executions())\n",
    "\n",
    "# print(10)\n",
    "# print(Workflow.list_workflows())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
